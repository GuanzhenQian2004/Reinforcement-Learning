{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "We import the necessary libraries for numerical computations, plotting, and progress tracking."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b586aceb9b693675"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Progress bar for loops"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.063272Z",
     "start_time": "2024-11-11T04:05:01.057867Z"
    }
   },
   "id": "97708128d4817a25",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define some constants that will be used throughout the simulation, such as the possible actions and the true state values for comparison."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89c83923d16c52e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1\n",
    "\n",
    "# True state values for states 1 to 5 (states 'A' to 'E')\n",
    "TRUE_VALUES = np.zeros(7)\n",
    "TRUE_VALUES[1:6] = np.arange(1, 6) / 6.0\n",
    "TRUE_VALUES[6] = 1  # The right terminal state has a value of 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.136186Z",
     "start_time": "2024-11-11T04:05:01.131806Z"
    }
   },
   "id": "4210692d1cc3b296",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a RandomWalkEnvironment class to simulate the environment. The environment consists of five non-terminal states labeled 'A' to 'E' (states 1 to 5), and two terminal states (state 0 and state 6). The agent starts at state 3 ('C') and can move left or right until it reaches a terminal state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec576db6c8d21a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RandomWalkEnvironment:\n",
    "    def __init__(self):\n",
    "        # Initialize the starting state and terminal states\n",
    "        self.start_state = 3  # Start at state 'C'\n",
    "        self.left_terminal_state = 0\n",
    "        self.right_terminal_state = 6\n",
    "        self.actions = [ACTION_LEFT, ACTION_RIGHT]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the environment to the starting state\n",
    "        self.current_state = self.start_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take an action and move to the next state\n",
    "        if action == ACTION_LEFT:\n",
    "            next_state = self.current_state - 1\n",
    "        elif action == ACTION_RIGHT:\n",
    "            next_state = self.current_state + 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        \n",
    "        # Check if the next state is terminal\n",
    "        done = next_state in [self.left_terminal_state, self.right_terminal_state]\n",
    "        reward = 0  # Rewards are zero until reaching a terminal state\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.154189Z",
     "start_time": "2024-11-11T04:05:01.147581Z"
    }
   },
   "id": "ec53506eabea6ba9",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ValueFunction class stores and updates the estimated values of each state. The initial values are set to 0.5 for non-terminal states, with the left terminal state at 0 and the right terminal state at 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8b80cfeeaa6590"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    def __init__(self, initial_values=None):\n",
    "        if initial_values is None:\n",
    "            # Initialize state values to 0.5 for non-terminal states\n",
    "            self.values = np.full(7, 0.5)\n",
    "            self.values[0] = 0  # Left terminal state\n",
    "            self.values[6] = 1  # Right terminal state\n",
    "        else:\n",
    "            # Use provided initial values\n",
    "            self.values = initial_values.copy()\n",
    "    \n",
    "    def update(self, state, delta):\n",
    "        # Update the value of a state by adding the delta\n",
    "        self.values[state] += delta\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        # Get the current estimated value of a state\n",
    "        return self.values[state]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.167186Z",
     "start_time": "2024-11-11T04:05:01.155198Z"
    }
   },
   "id": "33ebf4db9e8b904f",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Agent class represents the agent interacting with the environment. The agent can use either TD or MC methods to update the value function based on the episodes it plays."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "debbcea960203ed0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, value_function, alpha=0.1):\n",
    "        self.env = env\n",
    "        self.value_function = value_function\n",
    "        self.alpha = alpha  # Step size parameter\n",
    "    \n",
    "    def choose_action(self):\n",
    "        # Randomly choose an action (left or right) with equal probability\n",
    "        return np.random.choice(self.env.actions)\n",
    "    \n",
    "    def play_episode_td(self, batch=False):\n",
    "        # Play an episode using the TD method\n",
    "        return self.run_episode(batch, self.temporal_difference_update)\n",
    "    \n",
    "    def play_episode_mc(self, batch=False):\n",
    "        # Play an episode using the MC method\n",
    "        return self.run_episode(batch, self.monte_carlo_update)\n",
    "    \n",
    "    def run_episode(self, batch, update_func):\n",
    "        # General method to run an episode and update the value function\n",
    "        trajectory = []  # List to store the sequence of visited states\n",
    "        rewards = []     # List to store the rewards received\n",
    "        self.env.reset()\n",
    "        state = self.env.current_state\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        while True:\n",
    "            action = self.choose_action()\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            trajectory.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if not batch:\n",
    "                # Update the value function if not in batch mode\n",
    "                update_func(state, next_state, reward)\n",
    "            \n",
    "            if done:\n",
    "                break  # Episode ends when a terminal state is reached\n",
    "            state = next_state  # Move to the next state\n",
    "        \n",
    "        return trajectory, rewards\n",
    "    \n",
    "    def temporal_difference_update(self, state, next_state, reward):\n",
    "        # Update the value function using the TD(0) update rule\n",
    "        delta = self.alpha * (reward + self.value_function.get_value(next_state) - self.value_function.get_value(state))\n",
    "        self.value_function.update(state, delta)\n",
    "    \n",
    "    def monte_carlo_update(self, state, _, reward):\n",
    "        # Update the value function using the MC update rule\n",
    "        # Since rewards are zero until the terminal state, returns are either 0 or 1\n",
    "        returns = 1.0 if self.env.current_state == self.env.right_terminal_state else 0.0\n",
    "        delta = self.alpha * (returns - self.value_function.get_value(state))\n",
    "        self.value_function.update(state, delta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.191018Z",
     "start_time": "2024-11-11T04:05:01.182344Z"
    }
   },
   "id": "72bf4a850fa558a",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function runs episodes and collects the estimated state values at specified episodes for plotting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93e0b58961162dd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_state_values_over_episodes(agent, episodes, method):\n",
    "    values_over_episodes = []\n",
    "    for episode in range(1, max(episodes) + 1):\n",
    "        # Play an episode using the specified method\n",
    "        if method == 'TD':\n",
    "            agent.play_episode_td()\n",
    "        elif method == 'MC':\n",
    "            agent.play_episode_mc()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'TD' or 'MC'\")\n",
    "        \n",
    "        if episode in episodes:\n",
    "            # Record the value estimates at the specified episodes\n",
    "            values_over_episodes.append((episode, agent.value_function.values.copy()))\n",
    "    return values_over_episodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.199250Z",
     "start_time": "2024-11-11T04:05:01.193027Z"
    }
   },
   "id": "c7dce0ecfa25380",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function plots the estimated state values against the true state values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f43cc60d858bc35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_state_values(values_over_episodes, true_values):\n",
    "    for episode, values in values_over_episodes:\n",
    "        plt.plot([\"A\", \"B\", \"C\", \"D\", \"E\"], values[1:6], label=f'{episode} episodes')\n",
    "    plt.plot([\"A\", \"B\", \"C\", \"D\", \"E\"], true_values[1:6], label='True values')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Estimated Value')\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.210432Z",
     "start_time": "2024-11-11T04:05:01.202269Z"
    }
   },
   "id": "c5bea77bea42688a",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "These functions compute the RMS errors over multiple runs for different alpha values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16350f9335231b3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_rms_errors(agent_class, method, alphas, episodes, runs, true_values):\n",
    "    # Compute RMS errors for different alpha values\n",
    "    errors_list = []\n",
    "    for alpha in alphas:\n",
    "        errors = calculate_average_error(agent_class, method, alpha, episodes, runs, true_values)\n",
    "        errors_list.append(errors)\n",
    "    return np.array(errors_list)\n",
    "\n",
    "def calculate_average_error(agent_class, method, alpha, episodes, runs, true_values):\n",
    "    # Calculate the average RMS error over multiple runs\n",
    "    total_errors = np.zeros(episodes)\n",
    "    for _ in tqdm(range(runs), leave=False):\n",
    "        env = RandomWalkEnvironment()\n",
    "        value_function = ValueFunction()\n",
    "        agent = agent_class(env, value_function, alpha=alpha)\n",
    "        errors = run_episodes(agent, episodes, method, true_values)\n",
    "        total_errors += errors\n",
    "    average_errors = total_errors / runs\n",
    "    return average_errors\n",
    "\n",
    "def run_episodes(agent, episodes, method, true_values):\n",
    "    # Run episodes and record the RMS error at each episode\n",
    "    errors = []\n",
    "    for _ in range(episodes):\n",
    "        if method == 'TD':\n",
    "            agent.play_episode_td()\n",
    "        elif method == 'MC':\n",
    "            agent.play_episode_mc()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'TD' or 'MC'\")\n",
    "        \n",
    "        # Compute RMS error for non-terminal states (states 1 to 5)\n",
    "        error = np.sqrt(np.mean((agent.value_function.values[1:6] - true_values[1:6]) ** 2))\n",
    "        errors.append(error)\n",
    "    return np.array(errors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.224937Z",
     "start_time": "2024-11-11T04:05:01.217369Z"
    }
   },
   "id": "1ff728464088ba3e",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "These functions perform batch updating using all episodes collected so far until convergence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29d6c44a52d7c710"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def batch_updating(agent_class, method, episodes, alpha, runs, true_values):\n",
    "    total_errors = np.zeros(episodes)\n",
    "    for _ in tqdm(range(runs), leave=False):\n",
    "        env = RandomWalkEnvironment()\n",
    "        value_function = ValueFunction()\n",
    "        # Initialize state values to zero for batch updating\n",
    "        value_function.values[1:6] = 0\n",
    "        agent = agent_class(env, value_function, alpha=alpha)\n",
    "        errors = run_batch_updates(agent, episodes, method, true_values)\n",
    "        total_errors += errors\n",
    "    average_errors = total_errors / runs\n",
    "    return average_errors\n",
    "\n",
    "def run_batch_updates(agent, episodes, method, true_values):\n",
    "    errors = []\n",
    "    trajectories = []\n",
    "    rewards_list = []\n",
    "    for _ in range(episodes):\n",
    "        # Collect trajectories and rewards without updating the value function\n",
    "        if method == 'TD':\n",
    "            trajectory, rewards = agent.play_episode_td(batch=True)\n",
    "        elif method == 'MC':\n",
    "            trajectory, rewards = agent.play_episode_mc(batch=True)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'TD' or 'MC'\")\n",
    "        \n",
    "        trajectories.append(trajectory)\n",
    "        rewards_list.append(rewards)\n",
    "        \n",
    "        # Perform batch update using all collected data\n",
    "        agent.value_function = batch_update(agent.value_function, trajectories, rewards_list, agent.alpha, method)\n",
    "        \n",
    "        # Compute RMS error after batch update\n",
    "        error = np.sqrt(np.mean((agent.value_function.values[1:6] - true_values[1:6]) ** 2))\n",
    "        errors.append(error)\n",
    "    return np.array(errors)\n",
    "\n",
    "def batch_update(value_function, trajectories, rewards_list, alpha, method):\n",
    "    # Update the value function until convergence\n",
    "    while True:\n",
    "        updates = compute_updates(value_function, trajectories, rewards_list, method)\n",
    "        updates *= alpha\n",
    "        if np.sum(np.abs(updates)) < 1e-3:\n",
    "            break  # Convergence criterion met\n",
    "        for state in range(1, 6):  # Update non-terminal states\n",
    "            value_function.update(state, updates[state])\n",
    "    return value_function\n",
    "\n",
    "def compute_updates(value_function, trajectories, rewards_list, method):\n",
    "    # Compute the total updates for each state over all episodes\n",
    "    updates = np.zeros(7)\n",
    "    for trajectory, rewards in zip(trajectories, rewards_list):\n",
    "        for i in range(len(trajectory) - 1):\n",
    "            state = trajectory[i]\n",
    "            if method == 'TD':\n",
    "                # TD target: reward + value of next state\n",
    "                target = rewards[i] + value_function.get_value(trajectory[i + 1])\n",
    "            elif method == 'MC':\n",
    "                # MC target: total return (reward at terminal state)\n",
    "                target = rewards[i]\n",
    "            else:\n",
    "                raise ValueError(\"Method must be 'TD' or 'MC'\")\n",
    "            \n",
    "            # Accumulate updates\n",
    "            updates[state] += target - value_function.get_value(state)\n",
    "    return updates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.259618Z",
     "start_time": "2024-11-11T04:05:01.248480Z"
    }
   },
   "id": "d47290fd5f1f533c",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function plots the RMS errors for different alpha values and methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6c56671fdb4c7e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_rms_errors(td_alphas, mc_alphas, errors_td, errors_mc):\n",
    "    # Plot RMS errors for TD methods\n",
    "    for idx, alpha in enumerate(td_alphas):\n",
    "        plt.plot(errors_td[idx], linestyle='solid', label=f'TD, α = {alpha:.02f}')\n",
    "    # Plot RMS errors for MC methods\n",
    "    for idx, alpha in enumerate(mc_alphas):\n",
    "        plt.plot(errors_mc[idx], linestyle='dashdot', label=f'MC, α = {alpha:.02f}')\n",
    "    plt.xlabel('Walks/Episodes')\n",
    "    plt.ylabel('Empirical RMS Error (averaged over states)')\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.270821Z",
     "start_time": "2024-11-11T04:05:01.266543Z"
    }
   },
   "id": "d884481564cdb550",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function plots the RMS errors during batch updating for TD and MC methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bddc0373f9f2b38e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_batch_errors(errors_td, errors_mc):\n",
    "    plt.plot(errors_td, label='TD')\n",
    "    plt.plot(errors_mc, label='MC')\n",
    "    plt.xlabel('Walks/Episodes')\n",
    "    plt.ylabel('Empirical RMS Error (averaged over states)')\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.277231Z",
     "start_time": "2024-11-11T04:05:01.272828Z"
    }
   },
   "id": "28f661d22111576d",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we run the experiments and generate the plots to compare the performance of TD and MC methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e33c66432d8c471"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def example_6_2():\n",
    "    # Define true state values for comparison\n",
    "    true_values = np.zeros(7)\n",
    "    true_values[1:6] = np.arange(1, 6) / 6.0\n",
    "    true_values[6] = 1  # Right terminal state\n",
    "    \n",
    "    plt.figure(figsize=(10, 20))\n",
    "    \n",
    "    # Plot estimated state values over episodes\n",
    "    plt.subplot(3, 1, 1)\n",
    "    env = RandomWalkEnvironment()\n",
    "    value_function = ValueFunction()\n",
    "    agent = Agent(env, value_function)\n",
    "    episodes_to_plot = [0, 1, 10, 100]\n",
    "    values_over_episodes = compute_state_values_over_episodes(agent, episodes_to_plot, method='TD')\n",
    "    plot_state_values(values_over_episodes, true_values)\n",
    "    \n",
    "    # Plot RMS errors over episodes for different alpha values\n",
    "    plt.subplot(3, 1, 2)\n",
    "    td_alphas = [0.15, 0.1, 0.05]\n",
    "    mc_alphas = [0.01, 0.02, 0.03, 0.04]\n",
    "    episodes = 101  # Number of episodes to run\n",
    "    runs = 100      # Number of runs to average over\n",
    "    \n",
    "    errors_td = compute_rms_errors(Agent, 'TD', td_alphas, episodes, runs, true_values)\n",
    "    errors_mc = compute_rms_errors(Agent, 'MC', mc_alphas, episodes, runs, true_values)\n",
    "    \n",
    "    plot_rms_errors(td_alphas, mc_alphas, errors_td, errors_mc)\n",
    "    \n",
    "    # Plot RMS errors during batch updating\n",
    "    plt.subplot(3, 1, 3)\n",
    "    batch_episodes = 100\n",
    "    batch_alpha = 0.001\n",
    "    batch_runs = 100\n",
    "    \n",
    "    errors_td_batch = batch_updating(Agent, 'TD', batch_episodes, batch_alpha, batch_runs, true_values)\n",
    "    errors_mc_batch = batch_updating(Agent, 'MC', batch_episodes, batch_alpha, batch_runs, true_values)\n",
    "    \n",
    "    plot_batch_errors(errors_td_batch, errors_mc_batch)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('example_6_2.png')\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:05:01.342982Z",
     "start_time": "2024-11-11T04:05:01.334108Z"
    }
   },
   "id": "ea5f18d0abb8ee84",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    example_6_2()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T04:07:53.403916Z",
     "start_time": "2024-11-11T04:05:01.344991Z"
    }
   },
   "id": "e63f056d1c624cf2",
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
