{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a778471731f24e6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m softmax\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T18:33:17.548021Z",
     "start_time": "2024-12-08T18:33:16.206521Z"
    }
   },
   "id": "67029bc611ca6e4b",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short corridor environment (Example 13.1).\n",
    "\n",
    "States: 0, 1, 2, 3 (terminal)\n",
    "Actions: go_right (True) or go_left (False)\n",
    "Transitions:\n",
    "  - From state 0 or 2:\n",
    "      If go_right: state += 1\n",
    "      Else: state = max(0, state - 1)\n",
    "  - From state 1 (the tricky one):\n",
    "      If go_right: state -= 1\n",
    "      Else: state += 1\n",
    "\n",
    "Reward:\n",
    "  - At non-terminal transitions: -1\n",
    "  - At terminal state (state == 3): 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d015a18a04e86c6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ShortCorridor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, go_right):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        \n",
    "        Args:\n",
    "            go_right (bool): If True, attempt to move right; otherwise attempt to move left.\n",
    "        \n",
    "        Returns:\n",
    "            (reward, done): The reward for this step and whether the episode is finished.\n",
    "        \"\"\"\n",
    "        if self.state == 0 or self.state == 2:\n",
    "            if go_right:\n",
    "                self.state += 1\n",
    "            else:\n",
    "                self.state = max(0, self.state - 1)\n",
    "        else:  # self.state == 1\n",
    "            if go_right:\n",
    "                self.state -= 1\n",
    "            else:\n",
    "                self.state += 1\n",
    "\n",
    "        if self.state == 3:\n",
    "            # Terminal state\n",
    "            return 0, True\n",
    "        else:\n",
    "            return -1, False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6995aadcbefb27bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "REINFORCE agent with no baseline.\n",
    "\n",
    "Attributes:\n",
    "    theta (np.array): Policy parameter vector for two actions (left, right).\n",
    "    alpha (float): Step-size parameter for updating theta.\n",
    "    gamma (float): Discount factor.\n",
    "    x (np.array): Feature representation for the two actions.\n",
    "    rewards (list): Reward trajectory for the current episode.\n",
    "    actions (list): Actions taken during the current episode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16eff735396dda30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        # Initialize theta such that initial conditions correspond to left-epsilon greedy.\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # Feature representation: \n",
    "        # For two actions: left and right\n",
    "        # x[:,0] = features for \"left\" action\n",
    "        # x[:,1] = features for \"right\" action\n",
    "        self.x = np.array([[0, 1],\n",
    "                           [1, 0]])\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def get_pi(self):\n",
    "        \"\"\"\n",
    "        Compute the current policy probabilities for [left, right].\n",
    "        Add an epsilon to ensure non-determinism.\n",
    "        \"\"\"\n",
    "        h = np.dot(self.theta, self.x)\n",
    "        t = np.exp(h - np.max(h))\n",
    "        pmf = t / np.sum(t)\n",
    "\n",
    "        # Ensure non-determinism\n",
    "        imin = np.argmin(pmf)\n",
    "        epsilon = 0.05\n",
    "        if pmf[imin] < epsilon:\n",
    "            pmf[:] = 1 - epsilon\n",
    "            pmf[imin] = epsilon\n",
    "\n",
    "        return pmf\n",
    "\n",
    "    def get_p_right(self):\n",
    "        \"\"\"\n",
    "        Probability of choosing the 'right' action according to the current policy.\n",
    "        \"\"\"\n",
    "        return self.get_pi()[1]\n",
    "\n",
    "    def choose_action(self, reward):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current policy.\n",
    "        \n",
    "        Args:\n",
    "            reward (float or None): Reward from the previous step.\n",
    "        \n",
    "        Returns:\n",
    "            go_right (bool): Chosen action.\n",
    "        \"\"\"\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "        pmf = self.get_pi()\n",
    "        go_right = np.random.uniform() <= pmf[1]\n",
    "        self.actions.append(go_right)\n",
    "        return go_right\n",
    "\n",
    "    def episode_end(self, last_reward):\n",
    "        \"\"\"\n",
    "        Called at the end of an episode to perform the policy parameter update using REINFORCE.\n",
    "        \n",
    "        Args:\n",
    "            last_reward (float): The final reward of the episode (from the terminal state).\n",
    "        \"\"\"\n",
    "        self.rewards.append(last_reward)\n",
    "\n",
    "        # Compute returns G\n",
    "        G = np.zeros(len(self.rewards))\n",
    "        G[-1] = self.rewards[-1]\n",
    "        for i in range(2, len(G) + 1):\n",
    "            G[-i] = self.gamma * G[-i + 1] + self.rewards[-i]\n",
    "\n",
    "        gamma_pow = 1\n",
    "        for i in range(len(G)):\n",
    "            action_idx = 1 if self.actions[i] else 0\n",
    "            pmf = self.get_pi()\n",
    "            grad_ln_pi = self.x[:, action_idx] - np.dot(self.x, pmf)\n",
    "            update = self.alpha * gamma_pow * G[i] * grad_ln_pi\n",
    "            self.theta += update\n",
    "            gamma_pow *= self.gamma\n",
    "\n",
    "        # Reset episode data\n",
    "        self.rewards = []\n",
    "        self.actions = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T18:33:17.552038Z",
     "start_time": "2024-12-08T18:33:17.551533Z"
    }
   },
   "id": "dcccf261371d7a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "REINFORCE agent with a baseline (state-value estimator).\n",
    "\n",
    "Attributes:\n",
    "    alpha_w (float): Step-size parameter for updating the baseline w.\n",
    "    w (float): Current estimate of the state value for the start state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba00ded3351c8ee8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ReinforceBaselineAgent(ReinforceAgent):\n",
    "    def __init__(self, alpha, gamma, alpha_w):\n",
    "        super(ReinforceBaselineAgent, self).__init__(alpha, gamma)\n",
    "        self.alpha_w = alpha_w\n",
    "        self.w = 0\n",
    "\n",
    "    def episode_end(self, last_reward):\n",
    "        \"\"\"\n",
    "        Called at the end of an episode to perform the policy parameter update using REINFORCE with baseline.\n",
    "        \n",
    "        Args:\n",
    "            last_reward (float): The final reward of the episode (from the terminal state).\n",
    "        \"\"\"\n",
    "        self.rewards.append(last_reward)\n",
    "\n",
    "        # Compute returns G\n",
    "        G = np.zeros(len(self.rewards))\n",
    "        G[-1] = self.rewards[-1]\n",
    "        for i in range(2, len(G) + 1):\n",
    "            G[-i] = self.gamma * G[-i + 1] + self.rewards[-i]\n",
    "\n",
    "        gamma_pow = 1\n",
    "        for i in range(len(G)):\n",
    "            # Update baseline\n",
    "            self.w += self.alpha_w * gamma_pow * (G[i] - self.w)\n",
    "\n",
    "            # Update policy parameters\n",
    "            action_idx = 1 if self.actions[i] else 0\n",
    "            pmf = self.get_pi()\n",
    "            grad_ln_pi = self.x[:, action_idx] - np.dot(self.x, pmf)\n",
    "            update = self.alpha * gamma_pow * (G[i] - self.w) * grad_ln_pi\n",
    "            self.theta += update\n",
    "\n",
    "            gamma_pow *= self.gamma\n",
    "\n",
    "        # Reset episode data\n",
    "        self.rewards = []\n",
    "        self.actions = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29eb07f3617021c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run a trial of multiple episodes using a given agent generator.\n",
    "\n",
    "Args:\n",
    "    num_episodes (int): Number of episodes to run.\n",
    "    agent_generator (callable): A function that returns a new agent instance.\n",
    "\n",
    "Returns:\n",
    "    rewards (np.array): Array of total rewards obtained in each episode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b978fdfde2c363e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def trial(num_episodes, agent_generator):\n",
    "    env = ShortCorridor()\n",
    "    agent = agent_generator()\n",
    "    rewards = np.zeros(num_episodes)\n",
    "\n",
    "    for episode_idx in range(num_episodes):\n",
    "        rewards_sum = 0\n",
    "        reward = None\n",
    "        env.reset()\n",
    "\n",
    "        while True:\n",
    "            go_right = agent.choose_action(reward)\n",
    "            reward, episode_end = env.step(go_right)\n",
    "            rewards_sum += reward\n",
    "\n",
    "            if episode_end:\n",
    "                agent.episode_end(reward)\n",
    "                break\n",
    "\n",
    "        rewards[episode_idx] = rewards_sum\n",
    "\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-08T18:33:17.553895Z"
    }
   },
   "id": "ad3702708f973046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the experiment for both:\n",
    "  - REINFORCE without baseline\n",
    "  - REINFORCE with baseline\n",
    "\n",
    "Returns:\n",
    "    rewards (np.array): Shape (2, num_trials, num_episodes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c2085a3365c5f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_experiment(num_trials, num_episodes, alpha, gamma):\n",
    "    agent_generators = [\n",
    "        lambda: ReinforceAgent(alpha=alpha, gamma=gamma),\n",
    "        lambda: ReinforceBaselineAgent(alpha=alpha * 10, gamma=gamma, alpha_w=alpha * 100)\n",
    "    ]\n",
    "\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\n",
    "\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\n",
    "        for i in tqdm(range(num_trials), desc=f'Agent {agent_index+1} of {len(agent_generators)}'):\n",
    "            reward = trial(num_episodes, agent_generator)\n",
    "            rewards[agent_index, i, :] = reward\n",
    "    return rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-08T18:33:17.554904Z"
    }
   },
   "id": "44ed2eff3c5509ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the results given the rewards array.\n",
    "\n",
    "Args:\n",
    "    rewards (np.array): Shape (num_agents, num_trials, num_episodes)\n",
    "    num_episodes (int): Number of episodes\n",
    "    labels (list): Labels for each agent's data    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25e9e4cb9735f60b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_results(rewards, num_episodes, labels):\n",
    "    # Plot the reference line at -11.6\n",
    "    plt.axhline(y=-11.6, color='red', linestyle='dashed', label='-11.6')\n",
    "\n",
    "    # Plot each agent's average performance\n",
    "    for i, label in enumerate(labels):\n",
    "        mean_rewards = rewards[i].mean(axis=0)\n",
    "        plt.plot(np.arange(num_episodes) + 1, mean_rewards, label=label, linestyle='-', linewidth=2.0)\n",
    "\n",
    "    plt.ylabel('Total reward on episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Comparison of REINFORCE With and Without Baseline')\n",
    "    plt.grid(True)  # Turn on grid for better visibility\n",
    "    plt.savefig('figure_13_2.png')\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-08T18:33:17.555918Z"
    }
   },
   "id": "204e9ec079959caf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameters for the experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1610f294401f39d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_trials = 100\n",
    "num_episodes = 1000\n",
    "alpha = 2e-4\n",
    "gamma = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f0c07aa4947df4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run experiment and collect data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b48a8faa70a9219a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rewards = run_experiment(num_trials, num_episodes, alpha, gamma)\n",
    "labels = ['Reinforce without baseline', 'Reinforce with baseline']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T18:33:17.559926Z",
     "start_time": "2024-12-08T18:33:17.558928Z"
    }
   },
   "id": "355dcbc1fb192f5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot results separately, allowing for easy modifications of appearance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d28966061ff29b39"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results(rewards, num_episodes, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-08T18:33:17.559926Z"
    }
   },
   "id": "f565b5860955d126"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
